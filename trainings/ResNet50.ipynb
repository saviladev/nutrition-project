{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0387413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 📦 IMPORTACIONES\n",
    "# =========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# ⚙️ CONFIGURACIÓN DE GPU\n",
    "# =========================================================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        dev_names = [tf.config.experimental.get_device_details(g).get(\"device_name\", str(g)) for g in gpus]\n",
    "        print(f\"✅ GPUs detectadas: {len(gpus)} -> {dev_names}\")\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"   Logical GPUs: {len(logical_gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"⚠️ Error al configurar GPUs:\", e)\n",
    "else:\n",
    "    print(\"⚠️ No se detectó GPU, se usará CPU\")\n",
    "\n",
    "# --- Opcional: activar mixed precision ---\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "# (si usas esto, recuerda que la última capa Dense debe ser dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 📂 CARGAR RUTAS\n",
    "# =========================================================\n",
    "\n",
    "# Rutas actualizadas para la nueva estructura\n",
    "BASE_DIR = \"images\"\n",
    "META_DIR = \"meta/meta\"\n",
    "\n",
    "# Verificar que las rutas existen\n",
    "if os.path.exists(BASE_DIR):\n",
    "    print(f\"Se encontró la ruta: {BASE_DIR} exitosamente!\")\n",
    "if os.path.exists(META_DIR):\n",
    "    print(f\"Se encontró la ruta: {META_DIR} exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fdc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 📄 LECTURA DE SPLITS (train/test)\n",
    "# =========================================================\n",
    "def load_split(filename):\n",
    "    path = os.path.join(META_DIR, filename).replace(\"\\\\\", \"/\")\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: No se encontró el archivo {path}\")\n",
    "        return []\n",
    "\n",
    "train_files = load_split(\"train.txt\")\n",
    "test_files = load_split(\"test.txt\")\n",
    "\n",
    "print(f\"📊 Train muestras: {len(train_files)}\")\n",
    "print(f\"📊 Test muestras: {len(test_files)}\")\n",
    "\n",
    "if len(train_files) == 0 or len(test_files) == 0:\n",
    "    print(\"❌ Error: No se pudieron cargar los archivos de splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 📊 CREAR DATAFRAMES PARA TRAIN Y TEST\n",
    "# =========================================================\n",
    "def create_dataframe(file_list, base_dir):\n",
    "    \"\"\"Crear DataFrame con rutas de imágenes y etiquetas\"\"\"\n",
    "    data = []\n",
    "    for file_path in file_list:\n",
    "        # Extraer clase del nombre del archivo (formato: clase/imagen.jpg)\n",
    "        parts = file_path.split('/')\n",
    "        if len(parts) >= 2:\n",
    "            class_name = parts[0]\n",
    "            full_path = os.path.join(base_dir, file_path).replace(\"\\\\\", \"/\") + \".jpg\"\n",
    "            data.append({\n",
    "                'filename': full_path,\n",
    "                'class': class_name\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Crear DataFrames\n",
    "train_df = create_dataframe(train_files, BASE_DIR)\n",
    "test_df = create_dataframe(test_files, BASE_DIR)\n",
    "\n",
    "print(f\"📊 Train DataFrame shape: {train_df.shape}\")\n",
    "print(f\"📊 Test DataFrame shape: {test_df.shape}\")\n",
    "print(f\"📊 Número de clases: {train_df['class'].nunique()}\")\n",
    "print(f\"📊 Clases encontradas: {sorted(train_df['class'].unique())}\")\n",
    "\n",
    "# Verificar algunas imágenes\n",
    "print(\"\\n🔍 Verificando existencia de algunas imágenes:\")\n",
    "sample_files = train_df['filename'].head().tolist()\n",
    "for file_path in sample_files:\n",
    "    exists = os.path.exists(file_path)\n",
    "    print(f\"{'✅' if exists else '❌'} {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 🏗️ CONFIGURACIÓN DE GENERADORES DE DATOS\n",
    "# =========================================================\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dividir train_df en train y validation\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.2, \n",
    "    stratify=train_df['class'],  # Mantener proporciones de clases\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"📊 Train split: {len(train_split)} muestras\")\n",
    "print(f\"📊 Validation split: {len(val_split)} muestras\")\n",
    "\n",
    "# Data Augmentation reducido para entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Solo preprocessing para validación\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Solo preprocessing para test\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Crear generadores usando los splits separados\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_split,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_split,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_steps = len(train_generator)\n",
    "val_steps = len(val_generator)    \n",
    "test_steps = len(test_generator)  \n",
    "\n",
    "print(\"✅ Generadores corregidos:\")\n",
    "print(f\"📊 Train steps: {train_steps}\")\n",
    "print(f\"📊 Validation steps: {val_steps}\")\n",
    "print(f\"📊 Test steps: {test_steps}\")\n",
    "print(f\"📊 Total train images: {train_generator.samples}\")\n",
    "print(f\"📊 Total validation images: {val_generator.samples}\")\n",
    "print(f\"📊 Total test images: {test_generator.samples}\")\n",
    "print(f\"📊 Número de clases: {len(train_generator.class_indices)}\")\n",
    "\n",
    "# Verificar un batch\n",
    "try:\n",
    "    sample_batch = next(train_generator)\n",
    "    print(f\"✅ Batch de prueba: {sample_batch[0].shape}, {sample_batch[1].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al obtener batch de prueba: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 🏗️ CONSTRUCCIÓN DEL MODELO ResNet50\n",
    "# =========================================================\n",
    "NUM_CLASSES = len(train_generator.class_indices)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Cargar ResNet50 pre-entrenado sin la capa top\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(*IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Agregar capas personalizadas\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "predictions = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions', dtype='float32')(x)\n",
    "\n",
    "# Crear el modelo completo\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print(f\"✅ Modelo corregido con {NUM_CLASSES} clases\")\n",
    "print(f\"📊 Parámetros totales: {model.count_params():,}\")\n",
    "\n",
    "# Mostrar resumen de capas trainables/no trainables\n",
    "trainable_params = sum([tf.size(weight).numpy() for weight in model.trainable_weights])\n",
    "non_trainable_params = sum([tf.size(weight).numpy() for weight in model.non_trainable_weights])\n",
    "print(f\"📊 Parámetros entrenables: {trainable_params:,}\")\n",
    "print(f\"📊 Parámetros no entrenables: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba22a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 🎯 ENTRENAMIENTO POR FASES\n",
    "# =========================================================\n",
    "\n",
    "# ---- Fase 1: Entrenar solo el clasificador ----\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔒 FASE 1: Entrenando solo el clasificador\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Congelar ResNet50\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compilar para Fase 1\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks mejorados\n",
    "early_stop_fase1 = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "checkpoint_fase1 = ModelCheckpoint(\n",
    "    \"best_model_fase1.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Callback para reducir learning rate\n",
    "reduce_lr_fase1 = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Entrenar Fase 1\n",
    "print(\"🚀 Entrenando Fase 1 (solo capas densas)\")\n",
    "history_fase1 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stop_fase1, checkpoint_fase1, reduce_lr_fase1],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Fase 2: Fine-tuning ----\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔓 FASE 2: Fine-tuning \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Desbloquear gradualmente las capas de ResNet50\n",
    "trainable = False\n",
    "for layer in base_model.layers:\n",
    "    # Desbloquear a partir de 'conv5_block1_1_conv' (últimas capas del ResNet)\n",
    "    if layer.name == 'conv5_block1_1_conv':\n",
    "        trainable = True\n",
    "    layer.trainable = trainable\n",
    "\n",
    "print(f\"Capas desbloqueadas: {sum(1 for layer in base_model.layers if layer.trainable)}\")\n",
    "\n",
    "# Compilar para Fase 2 con learning rate más bajo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks para Fase 2\n",
    "early_stop_fase2 = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "checkpoint_fase2 = ModelCheckpoint(\n",
    "    \"best_model_fase2.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Callback para reducir learning rate en fase 2\n",
    "reduce_lr_fase2 = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=4,\n",
    "    min_lr=1e-8,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Entrenar Fase 2\n",
    "print(\"🚀 Entrenando Fase 2 (fine-tuning) - CORREGIDO...\")\n",
    "history_fase2 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop_fase2, checkpoint_fase2, reduce_lr_fase2],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ Entrenamiento completado!\")\n",
    "\n",
    "# =========================================================\n",
    "# 📊 EVALUACIÓN EN TEST SET\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 EVALUACIÓN FINAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model.load_weights(\"best_model_fase2.h5\")\n",
    "\n",
    "# Evaluar en test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_steps, verbose=1)\n",
    "\n",
    "print(f\"\\n🎯 RESULTADOS FINALES:\")\n",
    "print(f\"📊 Test Loss: {test_loss:.4f}\")\n",
    "print(f\"📊 Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predicciones detalladas\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Obtener clases reales\n",
    "test_generator.reset()\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "print(f\"\\n📊 Classification Report:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be477bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 💾 GUARDADO DEL MODELO FINAL\n",
    "# =========================================================\n",
    "model_path = \"model_resnet50_final.h5\"\n",
    "model.save(model_path)\n",
    "print(f\"✅ Modelo final guardado en: {model_path}\")\n",
    "\n",
    "# Guardar también los pesos por separado\n",
    "weights_path = \"model_resnet50_weights.h5\"\n",
    "model.save_weights(weights_path)\n",
    "print(f\"✅ Pesos guardados en: {weights_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
